---
title: "Intro to Decision Trees"
author: "Ram Narasimhan"
date: '`r Sys.Date()`<br><br><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img style="border-width:0" src="images/by-nc-sa-4.0-88x31.png" /></a>'
output:
  ioslides_presentation:
    css: css/ioslides.css
    logo: images/logo.jpg
    widescreen: yes
---

# A Hands-on Approach to Understanding Decision Trees


## Load a bunch of R packages

```{r message=F, warning=F}
library(rpart)
library(rpart.plot)
library(caret)
library(tree)
library(evtree)
library(randomForest)
```

##IRIS dataset

<div class="columns-2">

We will use the fairly ubiquitous `iris` dataset.

    - It has 150 rows, each row is one observation about a flower
    - There are 3 species of iris flowers
    - There are 4 columns of measurements.

### Each Species has 50 samples (flowers)

4 columns

    - "Sepal.Length" 
    - "Sepal.Width"  
    - "Petal.Length" 
    - "Petal.Width" 




(Creative Commons)

<img src="images/iris_versicolor_3.jpg" alt="iris" height="350">
</div>


## Let's understand the dataset (Boxplots) {.smaller}
```{r}
featurePlot(x=iris[,1:4], y=iris[,5], 
            plot="box", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")),
            auto.key=list(columns=3))

```
Note that setosa is relatively 'easier' to predict

## {.smaller}
`Denisty plots` can bring out hidden features in the data 
```{r}
featurePlot(x=iris[,1:4], y=iris[,5], 
            plot="density", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")),
            auto.key=list(columns=3))
```


##

```{r}
set.seed(50)
rows_to_train <- 50
train_rows = sample(1:150, rows_to_train)
train_rows
train = iris[train_rows , ]
test = iris[-train_rows , ]
```


## Use `Rpart` to build and visualize tree

We can take our training data and use the 4 columns to try and build a decision tree. 

```{r}
dtree <- rpart(Species ~ ., method="class", data=train)
```

What `rpart` is doing is finding the best variables to `split` on, that will separate the different species. The resulting tree is stored in the variable `dtree`

## Look at an ascii way of describing the tree
```{r}
dtree
```
A bit difficult to understand. But there are many good values to visualize this.

##

```{r}
plot(dtree)
text(dtree, pretty=0) #doesn't look good

```



##

```{r}
rpart.plot(dtree)
```

## 
```{r}
rpart.plot(dtree, type = 4, extra=102)
```

## 
```{r}
rpart.plot(dtree, type = 4, extra=101)
```
You can play around with different options.


## EV TREE has good visualizations

```{r}
evol_tree <- evtree(Species ~ ., method="class", data=train)
evol_tree#look at an ascii way of describing the tree

```

## EV TREE has good visualizations

```{r}
plot(evol_tree)
```

## How good are these dtrees at Predicting?

```{r}
tree_pred <- predict(dtree, test, type='class')
table(tree_pred, test[, 5])

```


## Is EVOL TREE a good predictor?

```{r}
evol_pred <- predict(evol_tree, test)
table(evol_pred, test[, 5])

```


## Let's bring in a random forest classifier
```{r}
rf <- randomForest(Species ~ ., data = train)
rf #pretty informative
```

## Plotting the random forest classifier
```{r}
plot(rf)
```


##

```{r}
rf_preds <- predict(rf, test)
table(rf_preds, test$Species)

```

## Compare all three

```{r}
mean(rf_preds == test$Species)
mean(tree_pred==test$Species)
mean(evol_pred == test$Species)

```









# Questions?




